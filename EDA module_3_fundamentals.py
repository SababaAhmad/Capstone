# -*- coding: utf-8 -*-
"""Module 3 Fundamentals.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gljFAamcd7lnPz10x4n05D9LgqiTmAn9

In this module we will examine how a company's financial performance affects its stock price.

## Install and imports
"""

pip install eod

! pip install pmdarima

pip install pykalman

# imports

import requests
from datetime import datetime, timedelta

import pandas as pd
import numpy as np

import yfinance as yf

# OLS/GLS
import statsmodels.api as sm
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import TimeSeriesSplit
from statsmodels.api import OLS, GLS, add_constant
from statsmodels.stats.diagnostic import het_breuschpagan

# ARIMA
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.arima.model import ARIMA
from pmdarima import auto_arima
from pykalman import KalmanFilter
from sklearn.metrics import make_scorer, mean_squared_error, r2_score

# LSTM
from keras.models import Sequential
from keras.layers import LSTM, Dense
from scipy.interpolate import CubicSpline

# visualizations
import matplotlib.pyplot as plt
import plotly.graph_objects as go

"""# Pre-Processing Functions"""

# Interpolate daily values given a column of quarterly values

def interpolate_daily_values(column):
  # Create a dataframe
  daily_df = pd.DataFrame(columns = ['Adj Close', 'Daily Avg'])

  for i in range(len(column)):
    a = column.index[i]

    idx_pos = prices.index.searchsorted(a)
    row_index = idx_pos + 1
    average_daily_value = column.iloc[i] / (row_index-len(daily_df))

    # Create a smaller dataframe with data for only quarter i that will be concatenated to the daily_df
    sub_df = pd.DataFrame(columns=['Adj Close', 'Daily Avg'])
    sub_df['Adj Close'] = prices.iloc[len(daily_df):row_index]
    sub_df['Daily Avg'] = average_daily_value

    daily_df = pd.concat([daily_df, sub_df])

  return daily_df

"""# OLS Functions"""

# heteroskedasticity function
def check_heteroskedasticity(dependent_var, independent_var):
  """
  Check for the presence of heteroskedasticity in a regression model.

  Parameters:
  dependent_var (pandas.Series): The dependent variable.
  independent_var (pandas.Series): The independent variable.

  Returns:
  lm_p_value (float): p-value from the Breusch-Pagan test.
  """

  # Fit the regression model
  model = sm.OLS(dependent_var, sm.add_constant(independent_var)).fit()

  residuals = model.resid

  # Perform Breusch-Pagan test for heteroskedasticity
  lm, lm_p_value, fvalue, f_p_value = het_breuschpagan(residuals, model.model.exog)

  if lm_p_value <= 0.05:
    response = ('Heteroskedasticity detected. Breusch-Pagan test p-value: ',round(lm_p_value,4))
  elif lm_p_value > 0.05:
    response = ('Heteroskedasticity not detected. Breusch-Pagan test p-value: ',round(lm_p_value,4))

  return response

def perform_ols(df, independent_var, dependent_var, n_splits=4):
  """
  Perform cross-validation using OLS regression on time-series data.

  Parameters:
  df (pandas.DataFrame): The dataframe containing the independent and dependent variables.
  independent_var (str): Name of the column for the independent variable.
  dependent_var (str): Name of the column for the dependent variable.
  n_splits (int): Number of splits for TimeSeriesSplit. Default is 4.

  Returns:
  result_df (pandas.DataFrame): Dataframe containing average metrics across folds.
  """
  X = df[independent_var]
  y = df[dependent_var]

  tscv = TimeSeriesSplit(n_splits=n_splits)

  adjusted_r_squared = []
  aic = []
  coef = []
  coef_p_value = []
  rmse = []

  for train_index, test_index in tscv.split(df):
    train_data = df.iloc[train_index]
    test_data = df.iloc[test_index]

    # Prepare data for OLS regression
    X_train = add_constant(train_data[independent_var])
    y_train = train_data[dependent_var]

    model = OLS(y_train, X_train).fit()

    # Extract metrics from model summary
    adjusted_r_squared.append(model.rsquared_adj)
    aic.append(model.aic)
    coef.append(model.params[independent_var])
    coef_p_value.append(model.pvalues[independent_var])

    # Predict on test data and compute RMSE
    X_test = add_constant(test_data[independent_var])
    y_pred = model.predict(X_test)
    rmse.append(np.sqrt(mean_squared_error(test_data[dependent_var], y_pred)))

  # Calculate average metrics across folds
  avg_adj_r_squared = round(sum(adjusted_r_squared) / len(adjusted_r_squared), 3)
  avg_aic = round(sum(aic) / len(aic), 3)
  avg_coef = round(sum(coef) / len(coef), 9)
  avg_coef_p_value = round(sum(coef_p_value) / len(coef_p_value), 5)
  avg_rmse = round(sum(rmse) / len(rmse), 5)

  result_df = pd.DataFrame({
    'Average Adjusted R-squared': [avg_adj_r_squared],
    'Average AIC': [avg_aic],
    'Average Change Coefficient': [avg_coef],
    'Average p-value for Change coefficient': [avg_coef_p_value],
    'Average RMSE': [avg_rmse]
  })

  result_df.index = [dependent_var]

  return result_df

def perform_scaled_ols(df, independent_var, dependent_var, n_splits=4):
  """
  Perform cross-validation using OLS regression on time-series data.

  Parameters:
  df (pandas.DataFrame): The dataframe containing the independent and dependent variables.
  independent_var (str): Name of the column for the independent variable.
  dependent_var (str): Name of the column for the dependent variable.
  n_splits (int): Number of splits for TimeSeriesSplit. Default is 4.

  Returns:
  result_df (pandas.DataFrame): Dataframe containing average metrics across folds.
  """
  tscv = TimeSeriesSplit(n_splits=n_splits)

  # Initialize lists to store metrics
  adjusted_r_squared = []
  aic = []
  coef = []
  coef_p_value = []
  rmse = []

  # Scale data
  scaler = MinMaxScaler(feature_range=(0, 1))
  scaled_array = scaler.fit_transform(df[[independent_var, dependent_var]])
  scaled_df = pd.DataFrame(data=scaled_array, columns=[independent_var, dependent_var])

  for train_index, test_index in tscv.split(scaled_df):
    train_data = scaled_df.iloc[train_index]
    test_data = scaled_df.iloc[test_index]

    # Prepare data for OLS regression
    X_train = add_constant(train_data[independent_var])
    y_train = train_data[dependent_var]

    model = OLS(y_train, X_train).fit()

    adjusted_r_squared.append(model.rsquared_adj)
    aic.append(model.aic)
    coef.append(model.params[independent_var])
    coef_p_value.append(model.pvalues[independent_var])

    # Predict on test data and compute RMSE
    X_test = add_constant(test_data[independent_var])
    y_pred = model.predict(X_test)
    rmse.append(np.sqrt(mean_squared_error(test_data[dependent_var], y_pred)))


  # Calculate average metrics across folds
  avg_adj_r_squared = round(sum(adjusted_r_squared) / len(adjusted_r_squared), 3)
  avg_aic = round(sum(aic) / len(aic), 3)
  avg_coef = round(sum(coef) / len(coef), 9)
  avg_coef_p_value = round(sum(coef_p_value) / len(coef_p_value), 5)
  avg_rmse = round(sum(rmse) / len(rmse), 5)

  result_df = pd.DataFrame({
    'Average Adjusted R-squared': [avg_adj_r_squared],
    'Average AIC': [avg_aic],
    'Average Coefficient': [avg_coef],
    'Average p-value for Coefficient': [avg_coef_p_value],
    'Average RMSE': [avg_rmse]
  })

  result_df.index = [dependent_var]

  return result_df

def perform_scaled_gls(df, independent_var, dependent_var, n_splits=4):
  """
  Perform cross-validation using GLS regression on time-series data if heteroskedasticity has been detected.

  Parameters:
  df (pandas.DataFrame): The dataframe containing the independent and dependent variables.
  independent_var (str): Name of the column for the independent variable.
  dependent_var (str): Name of the column for the dependent variable.
  n_splits (int): Number of splits for TimeSeriesSplit. Default is 4.

  Returns:
  result_df (pandas.DataFrame): Dataframe containing average metrics across folds.
  """
  tscv = TimeSeriesSplit(n_splits=n_splits)

  adjusted_r_squared = []
  aic = []
  coef = []
  coef_p_value = []
  rmse = []

  scaler = MinMaxScaler(feature_range=(0, 1))
  scaled_array = scaler.fit_transform(df[[independent_var, dependent_var]])
  scaled_df = pd.DataFrame(data=scaled_array, columns=[independent_var, dependent_var])

  # Perform cross-validation
  for train_index, test_index in tscv.split(scaled_df):
    train_data = scaled_df.iloc[train_index]
    test_data = scaled_df.iloc[test_index]

    # Prepare data for OLS regression
    X_train = add_constant(train_data[independent_var])
    y_train = train_data[dependent_var]

    model = GLS(y_train, X_train).fit()

    # Extract metrics from model summary
    adjusted_r_squared.append(model.rsquared_adj)
    aic.append(model.aic)
    coef.append(model.params[independent_var])
    coef_p_value.append(model.pvalues[independent_var])

    # Predict on test data and compute RMSE
    X_test = add_constant(test_data[independent_var])
    y_pred = model.predict(X_test)
    rmse.append(np.sqrt(mean_squared_error(test_data[dependent_var], y_pred)))


  # Calculate average metrics across folds
  avg_adj_r_squared = round(sum(adjusted_r_squared) / len(adjusted_r_squared), 3)
  avg_aic = round(sum(aic) / len(aic), 3)
  avg_coef = round(sum(coef) / len(coef), 9)
  avg_coef_p_value = round(sum(coef_p_value) / len(coef_p_value), 5)
  avg_rmse = round(sum(rmse) / len(rmse), 5)

  result_df = pd.DataFrame({
    'Average Adjusted R-squared': [avg_adj_r_squared],
    'Average AIC': [avg_aic],
    'Average Coefficient': [avg_coef],
    'Average p-value for Coefficient': [avg_coef_p_value],
    'Average RMSE': [avg_rmse]
  })

  result_df.index = [dependent_var]

  return result_df

"""# ARIMA Functions"""

def check_stationarity(time_series):
    """
    Check stationarity of a time series using the Augmented Dickey-Fuller (ADF) test.

    Parameters:
    - time_series: pandas Series, the time series data to be checked

    Returns:
    - stationarity: boolean, True if the time series is stationary, False otherwise
    """
    # Perform Augmented Dickey-Fuller test
    adf_result = adfuller(time_series)

    test_statistic, p_value, critical_values = adf_result[0], adf_result[1], adf_result[4]

    # Check stationarity based on p-value
    stationarity = p_value < 0.05  # If p-value is less than 0.05, reject the null hypothesis (non-stationary)

    return stationarity

# ARIMA model with an exogenous variable

def arima_exog(df, endog, exog):

  tscv = TimeSeriesSplit(n_splits=4)

  # Define the state transition matrix
  transition_matrix = [[1, 1], [0, 1]]

  # Define the observation matrix
  observation_matrix = [[1, 0]]

  # Initialize Kalman filter
  kf = KalmanFilter(transition_matrices=transition_matrix,
                    observation_matrices=observation_matrix)

  r_squared_values = []
  adjusted_r_squared_values = []
  mean_squared_errors = []
  aic_values = []
  rmse_values = []

  # Calculate R-squared
  def calculate_r_squared(y_true, y_pred):
      ss_res = np.sum((y_true - y_pred) ** 2)
      ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
      r_squared = 1 - (ss_res / ss_tot)
      return r_squared

  # Iterate over each split
  for train_index, test_index in tscv.split(df):
    train_data = df.iloc[train_index]
    train_data.dropna(inplace=True)
    test_data = df.iloc[test_index]

    # Fit Kalman filter to training data
    train_states = kf.em(train_data[endog].values, n_iter=5).smooth(train_data[endog].values)[0]

    # Extract filtered estimates of stock prices from Kalman filter
    train_filtered_prices = train_states[:, 0]

    # Use filtered estimates of stock prices as input for ARIMA model
    auto_model = auto_arima(train_filtered_prices, exogenous=train_data[[exog]],
                            trace=True, error_action='ignore', suppress_warnings=True)

    # Fit ARIMA model with optimal order
    model = ARIMA(endog=train_filtered_prices, exog=train_data[[exog]],
                  order=auto_model.order)
    result = model.fit()

    # Forecast future differences
    forecast_steps = len(test_data)
    forecast_diff = result.forecast(steps=forecast_steps, exog=test_data[[exog]])

    # Convert differenced forecasts back to original scale using Kalman filter estimates
    forecast_states = kf.filter(test_data[endog].values)[0]
    forecast_filtered_prices = forecast_states[:, 0]
    forecast = forecast_diff.cumsum() + forecast_filtered_prices[-1]

    # Calculate R-Squared
    y_pred_train_arima = result.predict()
    r_squared_train = calculate_r_squared(train_filtered_prices, y_pred_train_arima)
    r_squared_values.append(r_squared_train)

    n = len(test_data)
    p = len(result.params)
    adjusted_r_squared = 1 - ((1 - r_squared_train) * (n - 1) / (n - p - 1))
    adjusted_r_squared_values.append(adjusted_r_squared)

    # Calculate Mean Squared Error
    mse = mean_squared_error(test_data[endog], forecast)
    mean_squared_errors.append(mse)

    rmse = np.sqrt(mean_squared_error(test_data[endog], forecast))
    rmse_values.append(rmse)

    aic_values.append(result.aic)

  avg_r_squared = np.mean(r_squared_values)
  avg_adjusted_r_squared = np.mean(adjusted_r_squared_values)
  avg_mse = np.mean(mean_squared_errors)
  avg_aic = np.mean(aic_values)
  avg_rmse = np.mean(rmse_values)

  result_df = pd.DataFrame({'Average R-Squared': [avg_r_squared],
                               'Average Adjusted R-Squared': [avg_adjusted_r_squared],
                               'Average Mean Squared Error': [avg_mse],
                               'Average RMSE': [avg_rmse],
                               'Average AIC': [avg_aic]})

  result_df.index = [ticker_symbol]

  return result_df

# SARIMAX model with an exogenous variable

def sarimax_exog(df, endog, exog):

  tscv = TimeSeriesSplit(n_splits=4)

  # Define the state transition matrix
  transition_matrix = [[1, 1], [0, 1]]

  observation_matrix = [[1, 0]]

  # Initialize Kalman filter
  kf = KalmanFilter(transition_matrices=transition_matrix,
                    observation_matrices=observation_matrix)

  r_squared_values = []
  adjusted_r_squared_values = []
  mean_squared_errors = []
  aic_values = []
  rmse_values = []

  # Calculate R-squared
  def calculate_r_squared(y_true, y_pred):
      ss_res = np.sum((y_true - y_pred) ** 2)
      ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
      r_squared = 1 - (ss_res / ss_tot)
      return r_squared

  # Iterate over each split
  for train_index, test_index in tscv.split(df):
    train_data = df.iloc[train_index]
    train_data.dropna(inplace=True)
    test_data = df.iloc[test_index]

    # Fit Kalman filter to training data
    train_states = kf.em(train_data[endog].values, n_iter=5).smooth(train_data[endog].values)[0]

    # Extract filtered estimates of stock prices from Kalman filter
    train_filtered_prices = train_states[:, 0]

    # Use filtered estimates of stock prices as input for ARIMA model
    auto_model = auto_arima(train_filtered_prices, exogenous=train_data[[exog]],
                            trace=True, error_action='ignore', suppress_warnings=True)

    model = SARIMAX(endog=train_filtered_prices, exog=train_data[[exog]],
                  order=auto_model.order)
    result = model.fit()

    # Forecast future differences
    forecast_steps = len(test_data)
    forecast_diff = result.forecast(steps=forecast_steps, exog=test_data[[exog]])

    # Convert differenced forecasts back to original scale using Kalman filter estimates
    forecast_states = kf.filter(test_data[endog].values)[0]
    forecast_filtered_prices = forecast_states[:, 0]
    forecast = forecast_diff.cumsum() + forecast_filtered_prices[-1]

    y_pred_train_arima = result.predict()
    r_squared_train = calculate_r_squared(train_filtered_prices, y_pred_train_arima)
    r_squared_values.append(r_squared_train)

    n = len(test_data)
    p = len(result.params)
    adjusted_r_squared = 1 - ((1 - r_squared_train) * (n - 1) / (n - p - 1))
    adjusted_r_squared_values.append(adjusted_r_squared)

    # Calculate Mean Squared Error
    mse = mean_squared_error(test_data[endog], forecast)
    mean_squared_errors.append(mse)

    rmse = np.sqrt(mean_squared_error(test_data[endog], forecast))
    rmse_values.append(rmse)

    aic_values.append(result.aic)

  avg_r_squared = np.mean(r_squared_values)
  avg_adjusted_r_squared = np.mean(adjusted_r_squared_values)
  avg_mse = np.mean(mean_squared_errors)
  avg_rmse = np.mean(rmse_values)
  avg_aic = np.mean(aic_values)

  result_df = pd.DataFrame({'Average R-Squared': [avg_r_squared],
                               'Average Adjusted R-Squared': [avg_adjusted_r_squared],
                               'Average Mean Squared Error': [avg_mse],
                               'Average RMSE': [avg_rmse],
                               'Average AIC': [avg_aic]})

  result_df.index = [ticker_symbol]

  return result_df

"""# LSTM Functions"""

def lstm_model(independent_variable, dependent_variable):
  combined_data = pd.concat([independent_variable, dependent_variable], axis=1).dropna()
  scaler = MinMaxScaler()
  scaled_data = scaler.fit_transform(combined_data)

  X = scaled_data[:, 0].reshape(-1, 1, 1)
  y = scaled_data[:, 1]

  tscv = TimeSeriesSplit(n_splits=4)

  mse_scores = []
  r2_scores = []
  adjusted_r2_scores = []
  rmse_scores = []

  for train_index, test_index in tscv.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

    model = Sequential()
    model.add(LSTM(units=50, input_shape=(X_train.shape[1], 1)))
    model.add(Dense(units=1))
    model.compile(optimizer='adam', loss='mean_squared_error')

    model.fit(X_train, y_train, epochs=100, batch_size=32)

    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    r2 = r2_score(y_test, y_pred)
    adjusted_r2 = 1 - (1 - r2) * (len(y) - 1) / (len(y) - X.shape[1] - 1)

    mse_scores.append(mse)
    rmse_scores.append(rmse)
    r2_scores.append(r2)
    adjusted_r2_scores.append(adjusted_r2)

  average_mse = sum(mse_scores) / len(mse_scores)
  average_rmse = sum(rmse_scores) / len(rmse_scores)
  average_r2 = sum(r2_scores) / len(r2_scores)
  average_adjusted_r2 = sum(adjusted_r2_scores) / len(adjusted_r2_scores)

  result_df = pd.DataFrame({
      'Average MSE': [average_mse],
      'Average RMSE': [average_rmse],
      'Average R-Squared': [average_r2],
      'Average Adjusted R-Squared': [average_adjusted_r2]
  })

  result_df.index = [ticker_symbol]

  return result_df

"""# Download Data

## MSFT
"""

# Starting this analysis with Microsoft
ticker_symbol = 'MSFT'

start_date = '2004-01-01'
end_date = '2023-12-31'
prices = yf.download(ticker_symbol, start=start_date, end=end_date)
prices = pd.DataFrame(data = prices['Adj Close'])

# Resample daily prices to quarterly average to align with financials

prices.index = pd.to_datetime(prices.index)

# Resample to quarterly frequency by taking the mean of each quarter
q_prices = prices.resample('Q').mean()

# Fetch data from EODHD with registered API Key
url = f'https://eodhd.com/api/fundamentals/{ticker_symbol}.US?api_token=65d35d21248781.29690477&fmt=json'
data = requests.get(url).json()

from google.colab import files
import requests
import pandas as pd

api_key = ''
url = f'https://financialmodelingprep.com/api/v3/balance-sheet-statement/MSFT?period=quarter&apikey={api_key}'
data = requests.get(url).json()

df = pd.DataFrame(data)

# Save DataFrame to a CSV file
csv_filename = 'msft_q_balance_sheet.csv'
df.to_csv(csv_filename, index=False)

# Download the CSV file
files.download(csv_filename)

api_key = ''
url = f'https://financialmodelingprep.com/api/v3/income-statement-growth/MSFT?period=quarter&apikey={api_key}'
data = requests.get(url).json()

data[0]

df.tail()

# Income Statements to DataFrame
income_statements_data = data['Financials']['Income_Statement']['quarterly']

# Filtered dictionary containing only the desired keys
filtered_income_statements_data = {key: income_statements_data[key] for key in income_statements_data if '2023-12-31' >= key >= '2003-12-28'}

income_statements_df = pd.DataFrame(filtered_income_statements_data).T

income_statements_df.index = pd.to_datetime(income_statements_df.index)

income_statements_df = income_statements_df.sort_index(ascending=True)

"""## Other Tickers"""

tickers = ['^GSPC', 'MSFT', 'AAPL', 'EL', 'KR', 'RL', 'HAS', 'APA', 'CZR', 'MKTX', 'BR', 'BIIB', 'REGN']

"""# OLS/GLS Models

## MSFT Revenue
"""

# effect of revenue on price
revenue = pd.DataFrame(data = pd.to_numeric(income_statements_df['totalRevenue']))
revenue.reset_index(inplace = True)
revenue = revenue.rename(columns = {'index': 'Date'})
revenue = revenue.set_index('Date')
revenue = revenue.drop(revenue.index[0])

revenue.head()

# Merge the two DataFrames based on the nearest matching quarterly intervals
revenue_df = pd.merge_asof(q_prices, revenue, left_index=True, right_index=True)

"""## Quarterly Revenue"""

# Calculate the quarterly change in revenue
revenue_df['Revenue Change'] = revenue_df['totalRevenue'].pct_change()

# Calculate the quarterly change in average adjusted closing price
revenue_df['Price Change'] = revenue_df['Adj Close'].pct_change()
revenue_df.dropna(inplace = True)

# Shift the Price Change column by one quarter
revenue_df['Shifted Price Change'] = revenue_df['Price Change'].shift(-1)
revenue_df.dropna(inplace = True)

check_heteroskedasticity(revenue_df['Adj Close'], revenue_df['totalRevenue'])

perform_scaled_gls(revenue_df, 'totalRevenue', 'Adj Close')

# check for heteroskedasticity in revenue pct change vs price return
check_heteroskedasticity(revenue_df['Price Change'], revenue_df['Revenue Change'])

# OLS % change revenue vs % change price
perform_ols(revenue_df, 'Revenue Change', 'Price Change')

# check for heteroskedasticity in revenue pct change vs price return for following quarter
check_heteroskedasticity(revenue_df['Shifted Price Change'], revenue_df['Revenue Change'])

# OLS % change revenue vs % change price for the next quarter
perform_ols(revenue_df, 'Revenue Change', 'Shifted Price Change')

"""## Daily Revenue"""

daily_revenue = interpolate_daily_values(revenue['totalRevenue'])
daily_revenue.head()

check_heteroskedasticity(daily_revenue['Adj Close'], daily_revenue['Daily Avg'])

# Heteroskedasticity detected therefore use GLS model
perform_scaled_gls(daily_revenue, 'Daily Avg', 'Adj Close')

"""## MSFT Net Income

## Quarterly Income
"""

# effect of net income on price
income = pd.DataFrame(data = pd.to_numeric(income_statements_df['netIncome']))

# Merge the two DataFrames based on the nearest matching quarterly intervals
income_df = pd.merge_asof(q_prices, income, left_index=True, right_index=True)

# Calculate the quarterly change in revenue
income_df['Income Change'] = income_df['netIncome'].pct_change()

# Calculate the quarterly change in average adjusted closing price
income_df['Price Change'] = income_df['Adj Close'].pct_change()
income_df.dropna(inplace = True)

# Shift the Price Change column by one quarter
income_df['Shifted Price Change'] = income_df['Price Change'].shift(-1)
income_df.dropna(inplace = True)

income.head()

income_df.head(3)

check_heteroskedasticity(income_df['Adj Close'], income_df['netIncome'])

# GLS net income vs price

perform_scaled_gls(income_df, 'netIncome', 'Adj Close')

check_heteroskedasticity(income_df['Price Change'], income_df['Income Change'])

perform_ols(income_df, 'Income Change', 'Price Change')

check_heteroskedasticity(income_df['Shifted Price Change'], income_df['Income Change'])

perform_ols(income_df, 'Income Change', 'Shifted Price Change')

"""## Daily Income"""

daily_income = interpolate_daily_values(income['netIncome'])
daily_income.head()

check_heteroskedasticity(daily_income['Adj Close'], daily_income['Daily Avg'])

# Heteroskedasticity detected therefore use GLS model
perform_scaled_gls(daily_income, 'Daily Avg', 'Adj Close')

"""## MSFT EPS

## Quarterly EPS
"""

# Income Statements to DataFrame
earnings_history_data = data['Earnings']['History']

# Filtered dictionary containing only the desired keys
filtered_earnings_history_data = {key: earnings_history_data[key] for key in earnings_history_data if '2023-12-31' >= key >= '2004-03-31'}

# Convert to DataFrame
earnings_history_df = pd.DataFrame(filtered_earnings_history_data).T

# Convert index to datetime
earnings_history_df.index = pd.to_datetime(earnings_history_df.index)

# Sort rows from oldest to newest
earnings_history_df = earnings_history_df.sort_index(ascending=True)

earnings_history_df.head()

# effect of net earnings surpise on price
surprise = pd.DataFrame(data = pd.to_numeric(earnings_history_df['surprisePercent']))

# Merge the two DataFrames based on the nearest matching quarterly intervals
surprise_df = pd.merge_asof(q_prices, surprise, left_index=True, right_index=True)

# Calculate the quarterly change in revenue
surprise_df['Surprise Change'] = surprise_df['surprisePercent'].pct_change()

# Calculate the quarterly change in average adjusted closing price
surprise_df['Price Change'] = surprise_df['Adj Close'].pct_change()
surprise_df.dropna(inplace = True)

# Shift the Price Change column by one quarter
surprise_df['Shifted Price Change'] = surprise_df['Price Change'].shift(-1)
surprise_df.dropna(inplace = True)

surprise_df['Surprise Change'] = surprise_df['Surprise Change'].replace(float('inf'), np.nan)
surprise_df['Surprise Change'] = surprise_df['Surprise Change'].replace(float('-inf'), np.nan)

surprise_df['Surprise Change'].fillna(surprise_df['surprisePercent'], inplace=True)

# OLS for earnings surprise percent vs stock price

check_heteroskedasticity(surprise_df['Adj Close'], surprise_df['surprisePercent'])

perform_scaled_ols(surprise_df, 'surprisePercent', 'Adj Close')

# OLS for change in percent surprise vs change in stock price

check_heteroskedasticity(surprise_df['Price Change'], surprise_df['Surprise Change'])

perform_ols(surprise_df, 'Surprise Change', 'Price Change')

# OLS for change in percent surprise vs change in stock price for following quarter

check_heteroskedasticity(surprise_df['Shifted Price Change'], surprise_df['Surprise Change'])

perform_ols(surprise_df, 'Surprise Change', 'Shifted Price Change')

"""Alternate Hypothesis: Percentage Surprise is correlated to change in stock price for the day following the report date."""

# include the report date
surprise_date_df = pd.merge(surprise_df, earnings_history_df[['reportDate']], left_index=True, right_index=True, how='left')

# Convert reportDate column to datetime
surprise_date_df['reportDate'] = pd.to_datetime(surprise_date_df['reportDate'])

surprise_date_df.head(3)

# running this cell changes the length of the prices dataframe
prices_copy = prices.copy()
prices_copy['Daily Price Change'] = prices['Adj Close'].pct_change()

# Shift the Price Change column by one quarter
prices_copy['Shifted Price Change'] = prices_copy['Daily Price Change'].shift(-1)
prices_copy = prices_copy.dropna()

# Select desired columns and set index to reportDate
surprise_date_df_trim = surprise_date_df[['surprisePercent', 'Surprise Change', 'reportDate']].copy()
surprise_date_df_trim.set_index('reportDate', inplace=True)

# Merge the two dataframes based on index values
price_report_df = pd.merge(surprise_date_df_trim, prices_copy, left_index=True, right_index=True, how='inner')

# Multiply the values in the 'Daily Price Change' column by 100 to convert to the same scale as surprisePercent and Surprise Change
price_report_df['Daily Price Change'] *= 100

# Multiply the values in the 'Shifted Price Change' column by 100 to convert to the same scale as surprisePercent and Surprise Change
price_report_df['Shifted Price Change'] *= 100

price_report_df.head(3)

# OLS for surprisePercent vs daily price change

check_heteroskedasticity(price_report_df['Daily Price Change'], price_report_df['surprisePercent'])

# Homoskedastic errors therefore OLS (not GLS) model

perform_ols(price_report_df, 'surprisePercent', 'Daily Price Change')

# OLS for surprisePercent vs price change for the following day

check_heteroskedasticity(price_report_df['Shifted Price Change'], price_report_df['surprisePercent'])

# Homoskedastic errors therefore OLS (not GLS) model

perform_ols(price_report_df, 'surprisePercent', 'Shifted Price Change')

# OLS for change in surprisePercent vs price return

check_heteroskedasticity(price_report_df['Daily Price Change'], price_report_df['Surprise Change'])

# Homoskedastic errors therefore OLS (not GLS) model

perform_ols(price_report_df, 'Surprise Change', 'Daily Price Change')

# OLS for change in surprisePercent vs price return for the following day

check_heteroskedasticity(price_report_df['Shifted Price Change'], price_report_df['Surprise Change'])

# Homoskedastic errors therefore OLS (not GLS) model

perform_ols(price_report_df, 'Surprise Change', 'Shifted Price Change')

"""## Daily EPS"""

EPS = earnings_history_df[['epsActual', 'epsEstimate']]

daily_epsactual = interpolate_daily_values(EPS['epsActual'])
daily_epsactual.head()

daily_epsestimate = interpolate_daily_values(EPS['epsEstimate'])
daily_epsestimate.head()

check_heteroskedasticity(daily_epsactual['Adj Close'], daily_epsactual['Daily Avg'])

# Heteroskedasticity detected therefore use GLS model
perform_scaled_gls(daily_epsactual, 'Daily Avg', 'Adj Close')

daily_epsestimate = interpolate_daily_values(EPS['epsEstimate'])
check_heteroskedasticity(daily_epsestimate['Adj Close'], daily_epsestimate['Daily Avg'])

# Heteroskedasticity detected therefore use GLS model
perform_scaled_gls(daily_epsestimate, 'Daily Avg', 'Adj Close')

"""# ARIMA Models

## Quarterly Data
"""

print('Stationarity check\n\
Prices:',check_stationarity(revenue_df['Adj Close']),
'\nRevenue:',check_stationarity(revenue_df['totalRevenue']),
'\nPrice Returns:',check_stationarity(revenue_df['Price Change']),
'\nRevenue Growth:',check_stationarity(revenue_df['Revenue Change']),       )

arima_exog(revenue_df, 'Shifted Price Change', 'Revenue Change') # AIC -119, MSE 0.23

sarimax_exog(revenue_df, 'Shifted Price Change', 'Revenue Change') # AIC -118, MSE 0.199

print('Stationarity check\n\
Prices:',check_stationarity(income_df['Adj Close']),
'\nNet Income:',check_stationarity(income_df['netIncome']),
'\nPrice Returns:',check_stationarity(income_df['Price Change']),
'\nIncome Growth:',check_stationarity(income_df['Income Change']),       )

arima_exog(income_df, 'Shifted Price Change', 'Income Change') # AIC -119.8, MSE 0.26

sarimax_exog(income_df, 'Shifted Price Change', 'Income Change') # AIC -118.6, MSE 0.23

print('Stationarity check\nPercentage earnings surprise:',check_stationarity(surprise_df['surprisePercent']),
'\nChange in earnings surprise from previous quarter:',check_stationarity(surprise_df['Surprise Change']))

#effect of earnings surprise percentage on following quarter average stock price
arima_exog(surprise_df, 'Shifted Price Change', 'surprisePercent') # AIC -123, MSE 0.22

sarimax_exog(surprise_df, 'Shifted Price Change', 'surprisePercent') # AIC -124, MSE 0.22

#effect of change in earnings surprise from previous quarter on following quarter average stock price
arima_exog(surprise_df, 'Shifted Price Change', 'Surprise Change') # AIC -122.5, MSE 0.18

sarimax_exog(surprise_df, 'Shifted Price Change', 'Surprise Change') # AIC -123, MSE 0.17

print('stationarity check for daily price returns: ',check_stationarity(price_report_df['Daily Price Change']))

# ARIMA for surprisePercent vs daily price change
arima_exog(price_report_df, 'Daily Price Change', 'surprisePercent') # AIC 27.9, MSE 17.9

# SARIMAX for surprisePercent vs daily price change
sarimax_exog(price_report_df, 'Daily Price Change', 'surprisePercent') # AIC 28.6, MSE 12.8

# ARIMA for change in earnings surprise percent from previous quarter vs daily price change
arima_exog(price_report_df, 'Daily Price Change', 'Surprise Change') # AIC 26.8, MSE 16.3

# SARIMAX for change in earnings surprise percent from previous quarter vs daily price change
sarimax_exog(price_report_df, 'Daily Price Change', 'Surprise Change') # AIC 27.7, MSE 11.7

# effect of earnings surprise percent on the price return of the day following the report date
sarimax_exog(price_report_df, 'Shifted Price Change', 'surprisePercent') # AIC 47.7, MSE 115.6

# effect of the change in earnings surprise from the previous quarter on the price return of the day following the report date
sarimax_exog(price_report_df, 'Shifted Price Change', 'Surprise Change') # AIC 52.4, MSE 122

"""## Daily Interpolated Data"""

print('Stationarity check\n\
Daily Prices:',check_stationarity(daily_revenue['Adj Close']),
'\nDaily Revenue:',check_stationarity(daily_revenue['Daily Avg']))

arima_exog(daily_revenue, 'Adj Close', 'Daily Avg')

sarimax_exog(daily_revenue, 'Adj Close', 'Daily Avg')

print('Stationarity check\n\
Daily Prices:',check_stationarity(daily_income['Adj Close']),
'\nDaily Net Income:',check_stationarity(daily_income['Daily Avg']))

sarimax_exog(daily_income, 'Adj Close', 'Daily Avg')

print('Stationarity check\n\
Daily Prices:',check_stationarity(daily_epsactual['Adj Close']),
'\nDaily Daily Actual EPS:',check_stationarity(daily_epsactual['Daily Avg']))

sarimax_exog(daily_epsactual, 'Adj Close', 'Daily Avg')

print('Stationarity check\n\
Daily Prices:',check_stationarity(daily_epsestimate['Adj Close']),
'\nDaily Daily Estimated EPS:',check_stationarity(daily_epsestimate['Daily Avg']))

sarimax_exog(daily_epsestimate, 'Adj Close', 'Daily Avg')

"""# LSTM

## Quarterly Data
"""

lstm_model(revenue_df['totalRevenue'], revenue_df['Adj Close'])

lstm_model(revenue_df['Revenue Change'], revenue_df['Price Change'])

lstm_model(revenue_df['Revenue Change'], revenue_df['Shifted Price Change'])

lstm_model(income_df['netIncome'], income_df['Adj Close'])

lstm_model(income_df['Income Change'], income_df['Price Change'])

lstm_model(income_df['Income Change'], income_df['Shifted Price Change'])

lstm_model(surprise_df['surprisePercent'], surprise_df['Adj Close'])

lstm_model(surprise_df['Surprise Change'], surprise_df['Price Change'])

lstm_model(surprise_df['Surprise Change'], surprise_df['Shifted Price Change'])

price_report_df.head()

# Surprise percent vs price return on the day of the report
lstm_model(price_report_df['surprisePercent'], price_report_df['Daily Price Change'])

# Change in Surprise Percent vs Price return on the day of the report
lstm_model(price_report_df['Surprise Change'], price_report_df['Daily Price Change'])

# surprise percent vs price return on the day after the report
lstm_model(price_report_df['surprisePercent'], price_report_df['Shifted Price Change'])

# Change in surprise percent vs price return on the day after the report
lstm_model(price_report_df['Surprise Change'], price_report_df['Shifted Price Change'])

lstm_model(daily_revenue['Daily Avg'], daily_revenue['Adj Close'])

lstm_model(daily_income['Daily Avg'], daily_income['Adj Close'])

lstm_model(daily_epsactual['Daily Avg'], daily_epsactual['Adj Close'])

lstm_model(daily_epsestimate['Daily Avg'], daily_epsestimate['Adj Close'])

"""# Analysis and Conclusions

## MSFT

For MSFT the effects of revenue, net income, earnings per share and earnings surprise on the stock price were examined.
The original financial statements are in quarterly time intervals while the stock price data are in daily intervals. For consistency, the first set of analyses was performed by downsampling the daily stock price data to quarterly averages.

---


**Quarterly data**


*   Revenue vs Stock Price: On the bases of AIC and RMSE, the OLS model outperforms both the ARIMA and LSTM models, with an Adjusted R Squared of 0.58 and RMSE of 0.137.This model suggests that there is a statistically significant positive relationship between revenue and stock price
*   Net Income vs Stock price: While the OLS model displays a positive adjusted R-Squared, indicating that 17% of the variation in the stock price is explained by the linear model incorporating net income as the independent variable, p-value of the regression coefficient indicates that this positive relationship could have happened by chance and does not demonstrate a causal relationship. The SARIMAX model also displays a positive relationship among these variables.
* Earnings surprise vs quarterly average stock price: The OLS model suggests that 24.5% of price returns is explained by the change in the earnings surprise from the previous quarter. This model also has a relatively low RMSE (0.086) and statistically significant regression coefficient. The equivalent SARIMAX model also displays a positive relationship but a higher RMSE, while the LSTM model contains a negative adjusted R-Squared, raising concerns about its validity.
* Earnings surprise vs stock price on the day of or the day following report release date: the OLS models for this analysis once again show positive relationships but much higher RMSEs compared to other factors, suggesting low predictive ability.

**Daily Interpolated Data**
OLS/GLS models:
* Daily revenue vs stock price: other model statistics such as adjusted R-Squared, rvenue coefficient, RMSE, remained relatively similar but the AIC improved from -210 to -13974, suggesting a better fit.
* Daily net income vs stock price: this model has also has a better AIC score of -11645 compared to -176 for the quarterly average data, suggesting a better fit. This also indicates that there is a weaker relationship among Net Income and stock price (8.2% compared to 17.2%) than indicated by the quarterly average data. The regression coefficient in this model is also statistically significant.
* Daily Actual EPS vs stock price and Daily Estimated EPS vs stock price: Similar to the previous 2 daily interpolated regressions, these models also display a significantly better AIC at -13437 (compared to around -200 for the quarterly average regression models). These models indicate that 42% of the variation in stock price is related to changes in actual and expected EPS, with statistically significant coefficients and relatively low RMSEs.
* SARIMAX models: when using the daily interpolated data in the SARIMAX models, adjusted R-Squared was on average 0.977, AIC was 640 and RMSE was 35140 across all models. These metrics indicate potential overfitting and low predictive ability.
* LSTM models: using daily interpolated data did not improve model performance for the LSTM models compared to using quarterly average data. For all 17 analyses that were run using the LSTM model, every single one had a negative adjusted R Squared (-2.2 on average) but a moderate RMSE (0.185 on average). This shows that it's possible that the LSTM model is not well-suited to analyzing this type of data.
"""

import requests
api_key = ''
# url = f'https://financialmodelingprep.com/api/v3/earnings-surprise/MSFT?period=quarter&apikey={api_key}'
url = f'https://financialmodelingprep.com/api/v3/earnings-surprises/MSFT?apikey={api_key}'
data = requests.get(url).json()

import pandas as pd

df = pd.DataFrame(data)

from google.colab import files
# Save DataFrame to a CSV file
csv_filename = 'msft_cash_flow.csv'
df.to_csv(csv_filename, index=True)

# Download the CSV file
files.download(csv_filename)