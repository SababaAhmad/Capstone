# -*- coding: utf-8 -*-
"""Module 4: Volatility.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FWShSCANbor8Sxd3ZjNfhtZi_ze5fcdg

This module will examine how closely each stock in our subset is related to the CBOE VIX.

# Install and Import Modules
"""

import pandas as pd
import numpy as np
import yfinance as yf

from sklearn.model_selection import TimeSeriesSplit
from statsmodels.api import OLS, GLS, add_constant
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
import statsmodels.api as sm
from statsmodels.stats.diagnostic import het_breuschpagan

"""# Functions"""

# heteroskedasticity function
def check_heteroskedasticity_for_volatility(dependent_var, independent_var):
  """
  Check for the presence of heteroskedasticity in a regression model.

  Parameters:
  dependent_var (pandas.Series): The dependent variable.
  independent_var (pandas.Series): The independent variable.

  Returns:
  lm_p_value (float): p-value from the Breusch-Pagan test.
  """

  X = independent_var.dropna()  # Ensure independent variable is a copy
  y = dependent_var.dropna()

  # Ensure both X and y have the same length (length of the shorter series)
  min_length = min(len(X), len(y))
  X = X.iloc[-min_length:]
  y = y.iloc[-min_length:]
  model = sm.OLS(y, sm.add_constant(X)).fit()

  residuals = model.resid

  # Perform Breusch-Pagan test for heteroskedasticity
  lm, lm_p_value, fvalue, f_p_value = het_breuschpagan(residuals, model.model.exog)

  if lm_p_value <= 0.05:
    response = ('Heteroskedasticity detected. Breusch-Pagan test p-value: ',round(lm_p_value,4))
  elif lm_p_value > 0.05:
    response = ('Heteroskedasticity not detected. Breusch-Pagan test p-value: ',round(lm_p_value,4))

  return response

def perform_scaled_ols(df1, df2, independent_var, dependent_var, n_splits=4):
  """
  Perform cross-validation using OLS regression on time-series data.

  Parameters:
  df (pandas.DataFrame): The dataframe containing the independent and dependent variables.
  independent_var (str): Name of the column for the independent variable.
  dependent_var (str): Name of the column for the dependent variable.
  n_splits (int): Number of splits for TimeSeriesSplit. Default is 4.

  Returns:
  result_df (pandas.DataFrame): Dataframe containing average metrics across folds.
  """
  X = df1[independent_var].dropna()  # Ensure independent variable is a copy
  y = df2[dependent_var].dropna()

  # Ensure both X and y have the same length (length of the shorter series)
  min_length = min(len(X), len(y))
  X = X.iloc[-min_length:]
  y = y.iloc[-min_length:]

  # Scale the data
  scaler = MinMaxScaler(feature_range=(0, 1))
  scaled_array = scaler.fit_transform(pd.concat([X, y], axis=1))  # Scale together
  scaled_df = pd.DataFrame(data=scaled_array, columns=[independent_var, dependent_var])

  tscv = TimeSeriesSplit(n_splits=n_splits)

  # Initialize lists to store metrics
  adjusted_r_squared = []
  aic = []
  coef = []
  coef_p_value = []
  rmse = []

  for train_index, test_index in tscv.split(scaled_df):
    train_data = scaled_df.iloc[train_index]
    test_data = scaled_df.iloc[test_index]

    X_train = add_constant(train_data[independent_var])
    y_train = train_data[dependent_var]
    model = OLS(y_train, X_train).fit()

    # Extract metrics from model summary
    adjusted_r_squared.append(model.rsquared_adj)
    aic.append(model.aic)
    coef.append(model.params[independent_var])
    coef_p_value.append(model.pvalues[independent_var])

    # Predict on test data and compute RMSE
    X_test = add_constant(test_data[independent_var])
    y_pred = model.predict(X_test)
    rmse.append(np.sqrt(mean_squared_error(test_data[dependent_var], y_pred)))

  avg_adj_r_squared = round(sum(adjusted_r_squared) / len(adjusted_r_squared), 3)
  avg_aic = round(sum(aic) / len(aic), 3)
  avg_coef = round(sum(coef) / len(coef), 9)
  avg_coef_p_value = round(sum(coef_p_value) / len(coef_p_value), 5)
  avg_rmse = round(sum(rmse) / len(rmse), 5)

  result_df = pd.DataFrame({
    'Average Adjusted R-squared': [avg_adj_r_squared],
    'Average AIC': [avg_aic],
    'Average Change Coefficient': [avg_coef],
    'Average p-value for Change coefficient': [avg_coef_p_value],
    'Average RMSE': [avg_rmse]
  })

  result_df.index = [dependent_var]

  return result_df

def perform_scaled_gls(df1, df2, independent_var, dependent_var, n_splits=4):
  """
  Perform cross-validation using GLS regression on time-series data.

  Parameters:
  df (pandas.DataFrame): The dataframe containing the independent and dependent variables.
  independent_var (str): Name of the column for the independent variable.
  dependent_var (str): Name of the column for the dependent variable.
  n_splits (int): Number of splits for TimeSeriesSplit. Default is 4.

  Returns:
  result_df (pandas.DataFrame): Dataframe containing average metrics across folds.
  """
  X = df1[independent_var].dropna()  # Ensure independent variable is a copy
  y = df2[dependent_var].dropna()

  # Ensure both X and y have the same length (length of the shorter series)
  min_length = min(len(X), len(y))
  X = X.iloc[-min_length:]
  y = y.iloc[-min_length:]

  scaler = MinMaxScaler(feature_range=(0, 1))
  scaled_array = scaler.fit_transform(pd.concat([X, y], axis=1))  # Scale together
  scaled_df = pd.DataFrame(data=scaled_array, columns=[independent_var, dependent_var])

  tscv = TimeSeriesSplit(n_splits=n_splits)

  # Initialize lists to store metrics
  adjusted_r_squared = []
  aic = []
  coef = []
  coef_p_value = []
  rmse = []

  for train_index, test_index in tscv.split(scaled_df):
    train_data = scaled_df.iloc[train_index]
    test_data = scaled_df.iloc[test_index]

    X_train = add_constant(train_data[independent_var])
    y_train = train_data[dependent_var]

    model = GLS(y_train, X_train).fit()

    # Extract metrics from model summary
    adjusted_r_squared.append(model.rsquared_adj)
    aic.append(model.aic)
    coef.append(model.params[independent_var])
    coef_p_value.append(model.pvalues[independent_var])

    # Predict on test data and compute RMSE
    X_test = add_constant(test_data[independent_var])
    y_pred = model.predict(X_test)
    rmse.append(np.sqrt(mean_squared_error(test_data[dependent_var], y_pred)))

  avg_adj_r_squared = round(sum(adjusted_r_squared) / len(adjusted_r_squared), 3)
  avg_aic = round(sum(aic) / len(aic), 3)
  avg_coef = round(sum(coef) / len(coef), 9)
  avg_coef_p_value = round(sum(coef_p_value) / len(coef_p_value), 5)
  avg_rmse = round(sum(rmse) / len(rmse), 5)

  result_df = pd.DataFrame({
    'Average Adjusted R-squared': [avg_adj_r_squared],
    'Average AIC': [avg_aic],
    'Average Change Coefficient': [avg_coef],
    'Average p-value for Change coefficient': [avg_coef_p_value],
    'Average RMSE': [avg_rmse]
  })

  result_df.index = [dependent_var]

  return result_df

# heteroskedasticity function
def heteroskedasticity_and_regression(dependent_df, independent_df, dependent_var, independent_var):
  """
  Check for the presence of heteroskedasticity in a regression model.

  Parameters:
  dependent_var (pandas.Series): The dependent variable.
  independent_var (pandas.Series): The independent variable.

  Returns:
  lm_p_value (float): p-value from the Breusch-Pagan test.
  """

  X = independent_df[independent_var].dropna()  # Ensure independent variable is a copy
  y = dependent_df[dependent_var].dropna()

  # Ensure both X and y have the same length (length of the shorter series)
  min_length = min(len(X), len(y))
  X = X.iloc[-min_length:]
  y = y.iloc[-min_length:]
  model = sm.OLS(y, sm.add_constant(X)).fit()

  residuals = model.resid

  # Perform Breusch-Pagan test for heteroskedasticity
  lm, lm_p_value, fvalue, f_p_value = het_breuschpagan(residuals, model.model.exog)

  if lm_p_value <= 0.05:
    # response = ('Heteroskedasticity detected. Breusch-Pagan test p-value: ',round(lm_p_value,4))
    response = perform_scaled_gls(independent_df,dependent_df,independent_var,dependent_var,n_splits = 4)

  elif lm_p_value > 0.05:
    # response = ('Heteroskedasticity not detected. Breusch-Pagan test p-value: ',round(lm_p_value,4))
    response = perform_scaled_ols(independent_df,dependent_df,independent_var,dependent_var,n_splits = 4)

  return response

"""# Load Datasets"""

# Set starting and ending dates
start_date = '2020-01-01'
end_date = '2024-03-05'

# List of ticker symbols
tickers = ['^GSPC', 'MSFT', 'AAPL', 'EL', 'KR', 'RL', 'HAS', 'APA', 'CZR', 'MKTX', 'BR', 'BIIB', 'REGN']

prices = pd.DataFrame()

for ticker_symbol in tickers:
    price = yf.download(ticker_symbol, start=start_date, end=end_date)['Adj Close']
    prices[ticker_symbol] = price

# Set starting and ending dates
start_date = '2020-01-01'
end_date = '2024-03-05'

# List of ticker symbols
vix_tickers = ['^VIX1D', '^VIX9D', '^VIX', '^VIX3M', '^VIX6M']

# Create an empty DataFrame with a full date index
vix_df = pd.DataFrame(index=pd.date_range(start_date, end_date))

for vix_ticker in vix_tickers:
    try:
        price = yf.download(vix_ticker, start=start_date, end=end_date)['Adj Close']

        vix_df[vix_ticker] = price.reindex(vix_df.index, method='ffill').fillna(method='ffill')
    except Exception as e:
        print(f"Error downloading data for {vix_ticker}: {e}")

vix_df = round(vix_df, 4)

vix_df = vix_df.rename_axis('Date')

# Get the intersection of indices between prices and vix_df
common_index = prices.index.intersection(vix_df.index)

# Select rows from vix_df based on the common index
vix_df = vix_df.loc[common_index]

from google.colab import files
# Save DataFrame to a CSV file
csv_filename = 'vix_df.csv'
vix_df.to_csv(csv_filename, index=True)

# Download the CSV file
files.download(csv_filename)

"""# Perform Regression"""

gspc_df = pd.DataFrame()

for i in vix_df.columns:
  result_df = heteroskedasticity_and_regression(prices, vix_df, '^GSPC', i)
  result_df.index = [i]
  gspc_df = pd.concat([gspc_df, result_df])

gspc_df

msft_df = pd.DataFrame()

for i in vix_df.columns:
  result_df = heteroskedasticity_and_regression(prices, vix_df, 'MSFT', i)
  result_df.index = [i]
  msft_df = pd.concat([msft_df, result_df])
msft_df

aapl_df = pd.DataFrame()

for i in vix_df.columns:
  result_df = heteroskedasticity_and_regression(prices, vix_df, 'AAPL', i)
  result_df.index = [i]  # Call the function
  aapl_df = pd.concat([aapl_df, result_df])  # Concatenate and reset index
aapl_df

el_df = pd.DataFrame()

for i in vix_df.columns:
  result_df = heteroskedasticity_and_regression(prices, vix_df, 'EL', i)
  result_df.index = [i]  # Call the function
  el_df = pd.concat([el_df, result_df])  # Concatenate and reset index
el_df

kr_df = pd.DataFrame()

for i in vix_df.columns:
  result_df = heteroskedasticity_and_regression(prices, vix_df, 'KR', i)
  result_df.index = [i]  # Call the function
  kr_df = pd.concat([kr_df, result_df])  # Concatenate and reset index
kr_df

rl_df = pd.DataFrame()

for i in vix_df.columns:
  result_df = heteroskedasticity_and_regression(prices, vix_df, 'RL', i)
  result_df.index = [i]  # Call the function
  rl_df = pd.concat([rl_df, result_df])  # Concatenate and reset index
rl_df

has_df = pd.DataFrame()

for i in vix_df.columns:
  result_df = heteroskedasticity_and_regression(prices, vix_df, 'HAS', i)
  result_df.index = [i]  # Call the function
  has_df = pd.concat([has_df, result_df])  # Concatenate and reset index
has_df

apa_df = pd.DataFrame()

for i in vix_df.columns:
  result_df = heteroskedasticity_and_regression(prices, vix_df, 'APA', i)
  result_df.index = [i]  # Call the function
  apa_df = pd.concat([apa_df, result_df])  # Concatenate and reset index
apa_df

czr_df = pd.DataFrame()

for i in vix_df.columns:
  result_df = heteroskedasticity_and_regression(prices, vix_df, 'CZR', i)
  result_df.index = [i]  # Call the function
  czr_df = pd.concat([czr_df, result_df])  # Concatenate and reset index
czr_df

mktx_df = pd.DataFrame()

for i in vix_df.columns:
  result_df = heteroskedasticity_and_regression(prices, vix_df, 'MKTX', i)
  result_df.index = [i]  # Call the function
  mktx_df = pd.concat([mktx_df, result_df])  # Concatenate and reset index
mktx_df

br_df = pd.DataFrame()

for i in vix_df.columns:
  result_df = heteroskedasticity_and_regression(prices, vix_df, 'BR', i)
  result_df.index = [i]  # Call the function
  br_df = pd.concat([br_df, result_df])  # Concatenate and reset index
br_df

biib_df = pd.DataFrame()

for i in vix_df.columns:
  result_df = heteroskedasticity_and_regression(prices, vix_df, 'BIIB', i)
  result_df.index = [i]  # Call the function
  biib_df = pd.concat([biib_df, result_df])  # Concatenate and reset index
biib_df

regn_df = pd.DataFrame()

for i in vix_df.columns:
  result_df = heteroskedasticity_and_regression(prices, vix_df, 'REGN', i)
  result_df.index = [i]  # Call the function
  regn_df = pd.concat([regn_df, result_df])  # Concatenate and reset index
regn_df

"""# Best Models"""

df_list_short = [gspc_df,msft_df,aapl_df,kr_df,rl_df,has_df,apa_df,czr_df,mktx_df,br_df,biib_df,regn_df]
# tickers_short = ['^GSPC', 'MSFT', 'AAPL', 'KR', 'RL', 'HAS', 'APA', 'CZR', 'MKTX', 'BR', 'BIIB', 'REGN']
tickers_short = [MSFT]

# without EL modification

# Create an empty DataFrame to store the best model, based on AIC value
best_model = pd.DataFrame()

# Loop through each DataFrame in the list
for df in df_list_short:
  # Rename index and reset
  df = df.rename_axis('Vix Model').reset_index()

  # Find the row with the lowest "Average AIC"
  best_row = df.nsmallest(1, 'Average AIC')  # Use nsmallest for lowest value

  # Concatenate the best row to the result DataFrame
  best_model = pd.concat([best_model, best_row])

best_model['Tickers'] = tickers_short
best_model.set_index('Tickers', inplace = True)

# EL has the best AIC with an insignificant P Value, so the second best AIC will be used
el_df_short = pd.DataFrame(data = el_df.iloc[3])
el_df_short = el_df_short.T
el_df_short.index.name = 'Vix Model'
el_df_short = el_df_short.reset_index()
el_df_short.index = ['EL']
best_model = pd.concat([best_model, el_df_short])

best_model = best_model.sort_values(by='Average Adjusted R-squared', ascending=False)

best_model

"""From the best_model output above, we can see that majority of the tickers have the 30 day VIX as their best model. On average the adjusted R-Squared is 0.13, the regression coefficient is -0.1557 which shows weak but a statistically significant relationship between the VIX indices and stock price returns. It can be noted that MSFT and AAPL have the best AIC scores, further reinforcing the concept that higher market capitalization lends itself to better modelling. Also noteworthy is that the 1 day and 9 day VIX are not in the best_model output, suggesting that, while statisticaly significant, they can be outperformed by longer maturities of the index. Given this analysis, it can be said that VIX is a relevant factor when building investment strategies but should be supplemented with other considerations.

## 1 Day VIX

Only for **CZR** (high beta) and the market itself (**^GSPC**) is the regression statistically significant at the 5% level for the 1 Day VIX. It can therefore be said that the observed relationships are unlikely due to chance. For the market index, this was the worst performing model with the highest AIC and lowest Adjusted R-Squared. For CZR too this was the worst-performing model with the higest AIC and RMSE, but the also the highest Adjusted R-Squared and correlation coefficient.

# 9 Day VIX
"""

# Create an empty DataFrame to store the second rows
nine_day_df = pd.DataFrame()

# Loop through each DataFrame in the list
for df in df_list:
  # Extract the second row and append it to the temporary DataFrame
  nine_day = df.iloc[1]  # Access the second row using index 1
  nine_day_df = pd.concat([nine_day_df, nine_day.to_frame().T], ignore_index=True)  # Concatenate and transpose

# Rename the new DataFrame
nine_day_df.columns = df.columns  # Set column names based on the first DataFrame's columns

# Rename the result DataFrame for clarity
# nine_day_df = nine_day_df.rename_axis('Ticker', axis=0)  # Set a descriptive index
nine_day_df['Tickers'] = tickers
nine_day_df.set_index('Tickers', inplace = True)

nine_day_df

"""# LSTM"""

!pip install keras-tuner

import requests
from google.colab import files
from datetime import datetime
from tqdm import tqdm
import time

import pandas as pd
import numpy as np

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import TimeSeriesSplit
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.callbacks import EarlyStopping
import math
from sklearn.model_selection import train_test_split
import kerastuner
import tensorflow as tf
from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator
from keras_tuner.tuners import RandomSearch
from kerastuner import HyperModel, RandomSearch
from keras_tuner import HyperParameters
import keras_tuner

# import nasdaqdatalink
import yfinance as yf

class LSTMHyperModel(HyperModel):
  def __init__(self, input_shape):
    self.input_shape = input_shape

  def build(self, hp):
    model = Sequential()
    model.add(LSTM(units=hp.Int('units', min_value=32, max_value=512, step=32),
                    input_shape=self.input_shape,
                    return_sequences=True))
    model.add(Dropout(rate=hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.05)))
    model.add(LSTM(units=hp.Int('units', min_value=32, max_value=512, step=32), return_sequences=False))
    model.add(Dense(1, activation='linear'))
    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])
    return model

def lstm_model_with_tuning(df, target_column, feature_columns):
  if isinstance(feature_columns, str):
    feature_columns = [feature_columns]

  X = df[feature_columns].values
  y = df[target_column].values.reshape(-1, 1)
  num_features = len(feature_columns)
  input_shape = (1, num_features)

  tscv = TimeSeriesSplit(n_splits=4)
  rmse_scores = []

  for train_index, test_index in tscv.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

    scaler_X = MinMaxScaler()
    scaler_y = MinMaxScaler()
    X_train_scaled = scaler_X.fit_transform(X_train).reshape(-1, 1, num_features)
    X_test_scaled = scaler_X.transform(X_test).reshape(-1, 1, num_features)
    y_train_scaled = scaler_y.fit_transform(y_train)
    y_test_scaled = scaler_y.transform(y_test)

    model_builder = LSTMHyperModel(input_shape=input_shape)
    tuner = RandomSearch(model_builder,
                          objective='mse',
                          max_trials=10,
                          executions_per_trial=1,
                          directory='my_dir',
                          project_name='d')

    tuner.search(X_train_scaled, y_train_scaled, epochs=10, batch_size=64, validation_data=(X_test_scaled, y_test_scaled))
    best_model = tuner.get_best_models(num_models=1)[0]

    predictions_scaled = best_model.predict(X_test_scaled)
    predictions = scaler_y.inverse_transform(predictions_scaled)
    rmse = math.sqrt(mean_squared_error(y_test, predictions))
    rmse_scores.append(rmse)

  # Forecasting
  forecasts = []
  for i in range(1, 3):
    last_input = df[feature_columns].values[-i].reshape(1, -1)
    last_input_scaled = scaler_X.transform(last_input).reshape(-1, 1, num_features)
    forecast_scaled = best_model.predict(last_input_scaled)
    forecast = scaler_y.inverse_transform(forecast_scaled)
    forecasts.append(forecast[0][0])

  results_df = pd.DataFrame({
      'Average RMSE': [np.mean(rmse_scores)],
      'Forecast March 6, 2024': forecasts[1],
      'Forecast March 7, 2024': forecasts[0]
  })

  return results_df

vix_df['Adj Close'] = prices['MSFT']
vix_df.head()

small_df = vix_df.iloc[832:]

vix_model = lstm_model_with_tuning(small_df, 'Adj Close',['^VIX1D','^VIX', '^VIX9D','^VIX3M','^VIX6M'])

vix_model

from sklearn.ensemble import RandomForestRegressor

y = small_df['Adj Close']
X = small_df.drop('Adj Close', axis=1)

model = RandomForestRegressor()
model.fit(X, y)
feature_importances = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)
print(feature_importances)