# -*- coding: utf-8 -*-
"""Module 1 Past Prices.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hZB-WGE5GRxh6xbpnJNdtsUcuLKZZBnp

# How much of the variation in the current stock price is caused by past prices?

# Installs and Imports
"""

!pip install pmdarima

pip install keras-tuner --upgrade

# imports

import pandas as pd
import numpy as np
import yfinance as yf
from tqdm import tqdm
import time
from google.colab import files


# Preprocessing and evaluation
from sklearn.metrics import r2_score, mean_squared_error
from statsmodels.tools.eval_measures import rmse
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import MinMaxScaler

# ARIMA
import statsmodels.api as sm
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.arima_process import ArmaProcess
from statsmodels.tsa.holtwinters import ExponentialSmoothing
from pmdarima.arima import auto_arima
from statsmodels.tsa.statespace.kalman_filter import KalmanFilter

# LSTM
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from keras_tuner import RandomSearch, Objective

import plotly.graph_objects as go

"""# Functions"""

# Linear Regression

def calculate_variation_explained(df_column, shift):
    """
    Calculate the proportion of variation explained by past prices using time series cross-validation.

    Parameters:
        df_column (pandas.Series): The DataFrame column containing the adjusted closing prices.
        shift (int): The number of days to shift for lagged prices.

    Returns:
        float: The mean R-squared value across cross-validation folds.
    """
    # Calculate lagged (past) adjusted closing prices
    lagged_column = df_column.shift(shift)

    # Drop NaN values resulting from the lag operation
    df = pd.concat([df_column, lagged_column], axis=1).dropna()

    # Define the dependent variable (current adjusted closing price)
    y = df.iloc[:, 0]

    # Define the independent variable (lagged adjusted closing price)
    X = sm.add_constant(df.iloc[:, 1])

    # Initialize TimeSeriesSplit cross-validation with 4 splits
    tscv = TimeSeriesSplit(n_splits=4)

    # List to store R-squared values for each fold
    r_squared_values = []

    # Perform cross-validation
    for train_index, test_index in tscv.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        model = sm.OLS(y_train, X_train).fit()

        y_pred = model.predict(X_test)
        r_squared = r2_score(y_test, y_pred)
        r_squared_values.append(r_squared)


    mean_r_squared = round(np.mean(r_squared_values),3)

    return mean_r_squared

# Check time series stationarity on a time series before using an ARIMA model

def check_stationarity(time_series):
    """
    Check stationarity of a time series using the Augmented Dickey-Fuller (ADF) test.

    Parameters:
    - time_series: pandas Series, the time series data to be checked

    Returns:
    - stationarity: boolean, True if the time series is stationary, False otherwise
    """
    # Perform Augmented Dickey-Fuller test
    adf_result = adfuller(time_series)

    test_statistic, p_value, critical_values = adf_result[0], adf_result[1], adf_result[4]

    stationarity = p_value < 0.05  # If p-value is less than 0.05, reject the null hypothesis (non-stationary)

    return stationarity, p_value

def sarimax_model(time_series):
  adjusted_r_squared_list = []
  rmse_list = []
  aic_list = []

  if not isinstance(time_series, pd.Series):
    time_series = pd.Series(time_series)

  tscv = TimeSeriesSplit(n_splits=4)

  for train_index, test_index in tscv.split(time_series):
    train, test = time_series.iloc[train_index], time_series.iloc[test_index]

    auto_model = auto_arima(train, seasonal=True, m=12, stepwise=True, suppress_warnings=True,
                            error_action='ignore', trace=False)

    model = SARIMAX(train, order=auto_model.order, seasonal_order=auto_model.seasonal_order,
                    enforce_stationarity=False, enforce_invertibility=False)
    results = model.fit(disp=0)

    predictions = results.get_forecast(steps=len(test), dynamic=False)
    predictions_mean = predictions.predicted_mean

    predictions_mean.index = test.index

    rmse = np.sqrt(mean_squared_error(test, predictions_mean))
    r_squared = r2_score(test, predictions_mean)
    n = len(test)
    k = len(results.params)
    adjusted_r_squared = 1 - ((1 - r_squared) * (n - 1) / (n - k - 1))

    adjusted_r_squared_list.append(adjusted_r_squared)
    rmse_list.append(rmse)
    aic_list.append(results.aic)

  full_model = SARIMAX(time_series, order=auto_model.order, seasonal_order=auto_model.seasonal_order,
                        enforce_stationarity=False, enforce_invertibility=False)
  full_results = full_model.fit(disp=0)
  forecast = full_results.get_forecast(steps=2)
  forecast_mean = forecast.predicted_mean

  results_df = pd.DataFrame({
    'Average Adjusted R-Squared': [np.mean(adjusted_r_squared_list)],
    'Average RMSE': [np.mean(rmse_list)],
    'Average AIC': [np.mean(aic_list)],
    '1 Day Forecast': forecast_mean.iloc[0],
    '2 Day Forecast': forecast_mean.iloc[1]
  })

  results_df.index = [time_series.name]

  return results_df

def sarimax_model_log(time_series_base):
  """
  Creates a 2 day forecast using log returns of a time series

  Parameters:
  - time_series: pandas Series, the time series data to be checked

  Returns:
  - results_df: a one-line dataframe containing Adjusted R-Squared, RMSE, 1 Day Price Forecast and 2 Day Price Forecast
  """

  time_series = np.log(time_series_base/time_series_base.shift(1)).dropna()

  adjusted_r_squared_list = []
  rmse_list = []
  aic_list = []

  if not isinstance(time_series, pd.Series):
    time_series = pd.Series(time_series)

  tscv = TimeSeriesSplit(n_splits=4)

  for train_index, test_index in tscv.split(time_series):
    train, test = time_series.iloc[train_index], time_series.iloc[test_index]

    auto_model = auto_arima(train, seasonal=True, m=12, stepwise=True, suppress_warnings=True,
                            error_action='ignore', trace=False)

    model = SARIMAX(train, order=auto_model.order, seasonal_order=auto_model.seasonal_order,
                    enforce_stationarity=False, enforce_invertibility=False)
    results = model.fit(disp=0)

    predictions = results.get_forecast(steps=len(test), dynamic=False)
    predictions_mean = predictions.predicted_mean

    # Ensure the index matches between the test and predictions for proper comparison
    predictions_mean.index = test.index

    rmse = np.sqrt(mean_squared_error(test, predictions_mean))
    r_squared = r2_score(test, predictions_mean)
    n = len(test)
    k = len(results.params)
    adjusted_r_squared = 1 - ((1 - r_squared) * (n - 1) / (n - k - 1))

    adjusted_r_squared_list.append(adjusted_r_squared)
    rmse_list.append(rmse)
    aic_list.append(results.aic)

  full_model = SARIMAX(time_series, order=auto_model.order, seasonal_order=auto_model.seasonal_order,
                        enforce_stationarity=False, enforce_invertibility=False)
  full_results = full_model.fit(disp=0)
  forecast = full_results.get_forecast(steps=2)
  forecast_mean = forecast.predicted_mean

  last_price = time_series_base[-1]

  log_return_day1 = forecast_mean.iloc[0]
  log_return_day2 = forecast_mean.iloc[1]

  # Convert log returns to actual prices
  forecast_price_day1 = last_price * np.exp(log_return_day1)
  forecast_price_day2 = forecast_price_day1 * np.exp(log_return_day2)


  results_df = pd.DataFrame({
    'Average Adjusted R-Squared': [np.mean(adjusted_r_squared_list)],
    'Average RMSE': [np.mean(rmse_list)],
    'Average AIC': [np.mean(aic_list)],
    '1 Day Forecast': forecast_price_day1,
    '2 Day Forecast': forecast_price_day2
  })

  results_df.index = [time_series_base.name]

  return results_df

def lstm_forecast(data_series, n_steps=5, n_features=1):
  data = data_series.values if isinstance(data_series, pd.Series) else data_series

  def preprocess_data(data, n_steps):
    X, y = [], []
    for i in range(len(data) - n_steps):
      end_ix = i + n_steps
      seq_x, seq_y = data[i:end_ix], data[end_ix]
      X.append(seq_x)
      y.append(seq_y)
    return np.array(X), np.array(y)

  X, y = preprocess_data(data, n_steps)
  X = X.reshape((X.shape[0], X.shape[1], n_features))

  tscv = TimeSeriesSplit(n_splits=4)
  rmse_list = []
  adjusted_r_squared_list = []

  for train_index, test_index in tscv.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

    model = Sequential()
    model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))
    model.add(Dense(1))
    model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')

    model.fit(X_train, y_train, epochs=100, verbose=0)

    y_pred = model.predict(X_test, verbose=0).flatten()
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    rmse_list.append(rmse)

    r_squared = r2_score(y_test, y_pred)
    n = len(y_test)
    k = n_features
    adjusted_r_squared = 1 - ((1 - r_squared) * (n - 1) / (n - k - 1))
    adjusted_r_squared_list.append(adjusted_r_squared)

  # Forecasting
  forecast_input = data[-n_steps:].reshape((1, n_steps, n_features))
  forecast = [model.predict(forecast_input, verbose=0)[0][0]]
  forecast_input = np.append(forecast_input.flatten()[1:], forecast).reshape((1, n_steps, n_features))
  forecast.append(model.predict(forecast_input, verbose=0)[0][0])

  # Output results
  results_df = pd.DataFrame({
      'Average RMSE': [np.mean(rmse_list)],
      'Average Adjusted R-Squared': [np.mean(adjusted_r_squared_list)],
      '1 Day Forecast': forecast[0],
      '2 Day Forecast': forecast[1]
  })

  results_df.index = [data_series.name]

  return results_df

def lstm_forecast_with_tuning(data_series, n_steps=5, n_features=1):
  data = data_series.values if isinstance(data_series, pd.Series) else data_series

  def preprocess_data(data, n_steps):
    X, y = [], []
    for i in range(len(data) - n_steps):
      end_ix = i + n_steps
      seq_x, seq_y = data[i:end_ix], data[end_ix]
      X.append(seq_x)
      y.append(seq_y)
    return np.array(X), np.array(y)

  X, y = preprocess_data(data, n_steps)
  X = X.reshape((X.shape[0], X.shape[1], n_features))

  tscv = TimeSeriesSplit(n_splits=4)
  metrics_list = []

  for train_index, test_index in tscv.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

    def build_model(hp):
      model = Sequential()
      model.add(LSTM(units=hp.Int('input_unit', min_value=32, max_value=256, step=32),
                      return_sequences=True, input_shape=(n_steps, n_features)))
      for i in range(hp.Int('n_layers', 1, 1)):
          model.add(LSTM(units=hp.Int(f'lstm_{i}_units', min_value=32, max_value=256, step=32),
                          return_sequences=(i < hp.Int('n_layers', 1, 1) - 1)))
      model.add(Dropout(hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)))
      model.add(Dense(1, activation=hp.Choice('dense_activation', values=['relu', 'sigmoid'], default='relu')))
      model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['mse'])
      return model

    LOG_DIR = f"{int(time.time())}"
    tuner = RandomSearch(build_model,
                          objective=Objective("mse", direction="min"),
                          max_trials=5,
                          executions_per_trial=1,
                          directory=LOG_DIR)

    tuner.search(x=X_train, y=y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), verbose=0)

    best_model = tuner.get_best_models(num_models=1)[0]

    y_pred = best_model.predict(X_test)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r_squared = r2_score(y_test, y_pred)
    n = len(y_test)
    k = 1
    adjusted_r_squared = 1 - ((1 - r_squared) * (n - 1) / (n - k - 1))
    metrics_list.append((rmse, adjusted_r_squared))

  avg_rmse = np.mean([m[0] for m in metrics_list])
  avg_adjusted_r_squared = np.mean([m[1] for m in metrics_list])

  forecast_input = data[-n_steps:].reshape((1, n_steps, n_features))
  forecast = [best_model.predict(forecast_input)[0][0]]
  forecast_input = np.append(forecast_input.flatten()[1:], forecast).reshape((1, n_steps, n_features))
  forecast.append(best_model.predict(forecast_input)[0][0])

  results_df = pd.DataFrame({
      'Average RMSE': [avg_rmse],
      'Average Adjusted R-Squared': [avg_adjusted_r_squared],
      '1 Day Forecast': forecast[0],
      '2 Day Forecast': forecast[1]
  })

  results_df.index = [data_series.name]

  return results_df

def lstm_forecast_with_tuning_log(data_series_base, n_steps=5, n_features=1):
  if not isinstance(data_series_base, pd.Series):
    raise ValueError("data_series_base must be a pandas Series.")

  log_returns = np.log(data_series_base / data_series_base.shift(1)).dropna()
  data = log_returns.values

  def preprocess_data(data, n_steps):
    X, y = [], []
    for i in range(len(data) - n_steps):
      end_ix = i + n_steps
      seq_x, seq_y = data[i:end_ix], data[end_ix]
      X.append(seq_x)
      y.append(seq_y)
    return np.array(X), np.array(y)

  X, y = preprocess_data(data, n_steps)
  X = X.reshape((X.shape[0], X.shape[1], n_features))

  tscv = TimeSeriesSplit(n_splits=4)
  metrics_list = []

  for train_index, test_index in tscv.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

    def build_model(hp):
      model = Sequential()
      model.add(LSTM(units=hp.Int('input_unit', min_value=32, max_value=256, step=32),
                      return_sequences=True, input_shape=(n_steps, n_features)))
      for i in range(hp.Int('n_layers', 1, 1)):
          model.add(LSTM(units=hp.Int(f'lstm_{i}_units', min_value=32, max_value=256, step=32),
                          return_sequences=(i < hp.Int('n_layers', 1, 1) - 1)))
      model.add(Dropout(hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)))
      model.add(Dense(1, activation=hp.Choice('dense_activation', values=['relu', 'sigmoid'], default='relu')))
      model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['mse'])
      return model

    LOG_DIR = f"{int(time.time())}"
    tuner = RandomSearch(build_model,
                          objective=Objective("mse", direction="min"),
                          max_trials=5,
                          executions_per_trial=1,
                          directory=LOG_DIR)

    tuner.search(x=X_train, y=y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), verbose=0)

    best_model = tuner.get_best_models(num_models=1)[0]

    y_pred = best_model.predict(X_test)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r_squared = r2_score(y_test, y_pred)
    n = len(y_test)
    k = 1
    adjusted_r_squared = 1 - ((1 - r_squared) * (n - 1) / (n - k - 1))
    metrics_list.append((rmse, adjusted_r_squared))

  avg_rmse = np.mean([m[0] for m in metrics_list])
  avg_adjusted_r_squared = np.mean([m[1] for m in metrics_list])

  forecast_input = data[-n_steps:].reshape((1, n_steps, n_features))
  forecast = [best_model.predict(forecast_input)[0][0]]
  forecast_input = np.append(forecast_input.flatten()[1:], forecast).reshape((1, n_steps, n_features))
  forecast.append(best_model.predict(forecast_input)[0][0])

  last_price = data_series_base[-1]

  log_return_day1 = forecast[0]
  log_return_day2 = forecast[1]

  # Convert log returns to actual prices
  forecast_price_day1 = last_price * np.exp(log_return_day1)
  forecast_price_day2 = forecast_price_day1 * np.exp(log_return_day2)

  results_df = pd.DataFrame({
      'Average RMSE': [avg_rmse],
      'Average Adjusted R-Squared': [avg_adjusted_r_squared],
      '1 Day Forecast': forecast_price_day1,
      '2 Day Forecast': forecast_price_day2
  })

  results_df.index = [data_series_base.name]

  return results_df

"""# Download MSFT Data"""

ticker_symbol = 'MSFT'

# set start and end dates from Jan 1, 2004 to Dec 31, 2023
start='2004-01-01'
end='2023-12-31'

msft_price_data = yf.download(ticker_symbol, start=start, end=end)

msft_adj_close_prices = msft_price_data['Adj Close']

msft_df = pd.DataFrame(msft_adj_close_prices)

msft_df.head()

# Create Plotly figure
fig = go.Figure()

fig.add_trace(go.Scatter(x=msft_df.index, y=msft_df['Adj Close'], mode='lines', name=ticker_symbol))

fig.update_layout(title=f'Adjusted Closing Prices of {ticker_symbol} (Jan 1, 2004 - Dec 31, 2023)',
                  xaxis_title='Date',
                  yaxis_title='Adjusted Closing Price')
fig.show()

"""# Linear Regression

To analyze how much of the variation in the price is caused by past prices, a regression analysis can be performed using past prices as explanatory variables and current prices as dependent variables. This will help assess the proportion of variation explained by past prices.
"""

calculate_variation_explained(msft_df['Adj Close'], 1)

"""This calculation shows that 99% of the variation in price of MSFT is explained by the price of the previous day."""

print("Previous 10 days: ",calculate_variation_explained(msft_df['Adj Close'], 10),"\nPrevious 2 months: ",calculate_variation_explained(msft_df['Adj Close'], 42))

"""This calculation shows that the previous 10 days of prices explain 93% of the variation in MSFT prices while the previous 2 months explains 68% of the variation. Let's look at these values in further detail:"""

# results_df = pd.DataFrame(columns=['Shift', 'R-squared'])

# for shift in range(1, 151):
#     r_squared = calculate_variation_explained(msft_df['Adj Close'], shift)
#     results_df = results_df.append({'Shift': shift, 'R-squared': r_squared}, ignore_index=True)
results = []

for shift in range(1, 151):
    r_squared = calculate_variation_explained(msft_df['Adj Close'], shift)
    results.append({'Shift': shift, 'R-squared': r_squared})

results_df = pd.DataFrame(results)

# Create a Plotly figure
fig = go.Figure()

fig.add_trace(go.Scatter(x=results_df['Shift'], y=results_df['R-squared'], mode='lines', name='R-squared'))

fig.update_layout(title=f'Percentage of Variation Explained by Number of Previous Trading Days for {ticker_symbol}',
                  xaxis_title='Number of Previous Trading Days',
                  yaxis_title='Percentage of Explained Variation')

fig.show()

"""This chart shows that past prices have a diminishing effect on the current price of MSFT for roughly 112 trading days, or approximately 5 months.

Performing this exercise on a simple cross section of 12 tickers from the S&P 500, along with the index itself:

*   2 large-cap: MSFT, AAPL
*   2 mid-cap: EL, KR
* 2 small-cap: RL, HAS
* 2 high beta: APA, CZR
* 2 beta neutral: MKTX, BR
* 2 low beta: BIIB, REGN
"""

# List of ticker symbols
tickers = ['^GSPC', 'MSFT', 'AAPL', 'EL', 'KR', 'RL', 'HAS', 'APA', 'CZR', 'MKTX', 'BR', 'BIIB', 'REGN']
df = pd.DataFrame()

for ticker_symbol in tickers:
    price_data = yf.download(ticker_symbol, start=start, end=end)
    df[ticker_symbol] = price_data['Adj Close']

variation_explained_dict = {}

for shift in range(1, 101):
    variation_explained_tickers = {}

    for ticker_symbol in df.columns:
        r_squared = calculate_variation_explained(df[ticker_symbol], shift)

        variation_explained_tickers[ticker_symbol] = r_squared

    variation_explained_dict[shift] = variation_explained_tickers

# Convert the variation explained dictionary to a DataFrame
variation_explained_df = pd.DataFrame(variation_explained_dict)
variation_explained_df = variation_explained_df.transpose()

# Create a Plotly figure
fig = go.Figure()

# Add line traces for each ticker symbol
for ticker_symbol in variation_explained_df.columns:
    fig.add_trace(go.Scatter(x=variation_explained_df.index, y=variation_explained_df[ticker_symbol], mode='lines', name=ticker_symbol))

# Update layout
fig.update_layout(title='Percentage of Variation Explained by Number of Previous Trading Days',
                  xaxis_title='Number of Previous Trading Days',
                  yaxis_title='Percentage of Explained Variation',
                  legend_title='Ticker')

# Show the plot
fig.show()

"""This exercise shows that for all the stocks in this simple cross-section, almost roughly 99% of the price variation is explained by the previous days price. <br> Extending the lag time frame to one month, while the average value (represented by ^GSPC) shows 87% of the variation explained by the previous month's prices, the large cap stock (AAPL) and beta-neutral stock (MKTX)shows the highest level of explained variation at 92% and 90% respectively, while the high beta stock (CZR) shows the lowest at 45%.<br> Looking at 3 months of lagged prices, the S&P 500 Index shows a roughly 62% explained variation, while the large cap (AAPL)beta-neutral (BR) shows the highest once again at 74% and 64% respectively. The high beta stock, CZR, shows a negative R-Squared value, indicating that prices prior to 37 trading days are not related to current prices.

These results are expected because large-cap stocks belong to well established companies with more stable fundamentals. Investors typically have more confidence in them. These stocks also attract more attention from analysts and investors so their information is more widely available, which leads to higher market efficiency and a stronger relationship between past and current prices.<br> Similarly, beta neutral stocks are less sensitive to overall market movements, which results in a stronger relationship between past and present prices. <br> In contrast, the high beta stocks are more volatile and sensitive to market fluctuations and sentiment.

# ARIMA
"""

# is MSFT stationary?
check_stationarity(msft_df)

"""The P-value of MSFT price data is greater than 0.05, meaning that the prices are not stationary. Therefore they need to be differenced."""

msft_df['Log Returns'] = np.log(msft_df['Adj Close'] / msft_df['Adj Close'].shift(1))

sarimax_model(msft_df['Log Returns'].dropna())

last_price = msft_df['Adj Close'].iloc[-1]


# Forecasted log returns
log_return_day1 = 0.000708
log_return_day2 = 0.000898

# Convert log returns to actual prices
forecast_price_day1 = last_price * np.exp(log_return_day1)
forecast_price_day2 = forecast_price_day1 * np.exp(log_return_day2)  # Note that we use the first forecast as the base for the second

# Print the forecasted prices
forecast_price_day1, forecast_price_day2

msft_10_yr = msft_df.iloc[2517:]
# sarimax_model(msft_10_yr['Log Returns'].dropna())

"""## Testing SARIMAX model on cross-section of tickers"""

log_returns_df = np.log(df / df.shift(1)).dropna()

# Create an empty DataFrame to store SARIMAX results
sarimax_results_df = pd.DataFrame()

# Iterate through each column in log_returns_df and apply the sarimax_model_log function
# for column in tqdm(df.columns, desc='Processing columns'):
#     time_series = df[column]
#     result_df = sarimax_model_log(time_series)
#     sarimax_results_df = pd.concat([sarimax_results_df, result_df])

# # Save DataFrame to a CSV file
# csv_filename = 'past_prices_sarimax_results_df.csv'
# sarimax_results_df.to_csv(csv_filename)

# # Download the CSV file
# files.download(csv_filename)

# Upload a CSV file
uploaded = files.upload()

# Check the uploaded files
for filename in uploaded.keys():
    print(f'Uploaded file: {filename}')

sarimax_results_df = pd.read_csv('past_prices_sarimax_results_df.csv')

sarimax_results_df

"""Average RMSE for 1 day forecasts: 7.3

Average RMSE for 2 day forecasts: 12.7

# LSTM
"""

lstm_forecast(msft_df['Adj Close'])

lstm_forecast(msft_df['Log Returns'].dropna())

lstm_forecast(msft_10_yr['Log Returns'].dropna())

last_price = msft_df['Adj Close'].iloc[-1]

# Forecasted log returns
log_return_day1 = 0.000071
log_return_day2 =0.000071

# Convert log returns to actual prices
forecast_price_day1 = last_price * np.exp(log_return_day1)
forecast_price_day2 = forecast_price_day1 * np.exp(log_return_day2)  # Note that we use the first forecast as the base for the second

# Print the forecasted prices
forecast_price_day1, forecast_price_day2

lstm_forecast_with_tuning(msft_df['Log Returns'].dropna())

lstm_forecast_with_tuning(msft_10_yr['Log Returns'].dropna())

"""## Testing LSTM With Tuning on Cross-Section of Tickers"""

# lstm_results_df = pd.DataFrame()

# for column in tqdm(df.columns, desc='Processing columns'):
#     time_series = df[column]
#     result_df = lstm_forecast_with_tuning_log(time_series)

#     # After getting the result, save it to a CSV with the column name as the file name
#     csv_filename = f'{column}_lstm.csv'
#     result_df.to_csv(csv_filename)

#     # Download the CSV file
#     files.download(csv_filename)

#     # Concatenate result_df to the lstm_results_df DataFrame
#     lstm_results_df = pd.concat([lstm_results_df, result_df])

# Upload a CSV file
uploaded = files.upload()

# Check the uploaded files
for filename in uploaded.keys():
    print(f'Uploaded file: {filename}')

lstm_results_df = pd.read_csv('past_prices_lstm_results_df.csv')

lstm_results_df.head()

"""Average RMSE for 1 day forecasts: 7.1  

Average RMSE for 2 day forecasts:12.4
"""